"""
Unified AI Service for OpenAI, Anthropic, and Ollama
"""
import openai
import anthropic
import requests
from typing import Dict, Optional, Any
from ..models import LLMProvider, AIModel
import logging

logger = logging.getLogger(__name__)


class AIService:
    """Unified service for interacting with different LLM providers"""
    
    def __init__(self, model: AIModel):
        self.model = model
        self.provider = model.provider
    
    def generate_fix_suggestion(
        self,
        error_context: str,
        code_context: Optional[str] = None,
        log_content: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Generate a fix suggestion based on error and code context
        
        Args:
            error_context: The error message or stack trace
            code_context: Relevant code from GitLab repository
            log_content: Full log content for additional context
        
        Returns:
            Dict with 'suggestion', 'explanation', 'confidence'
        """
        prompt = self._build_prompt(error_context, code_context, log_content)
        
        if self.provider.provider_type == 'openai':
            return self._call_openai(prompt)
        elif self.provider.provider_type == 'anthropic':
            return self._call_anthropic(prompt)
        elif self.provider.provider_type == 'ollama':
            return self._call_ollama(prompt)
        else:
            raise ValueError(f"Unsupported provider type: {self.provider.provider_type}")
    
    def _build_prompt(
        self,
        error_context: str,
        code_context: Optional[str],
        log_content: Optional[str]
    ) -> str:
        """Build the prompt for the LLM"""
        prompt_parts = [
            "You are an expert software engineer analyzing an error in a codebase.",
            "Your task is to identify the root cause and suggest a fix.",
            "",
            "=== ERROR CONTEXT ===",
            error_context,
        ]
        
        if code_context:
            prompt_parts.extend([
                "",
                "=== RELEVANT CODE ===",
                code_context,
            ])
        
        if log_content:
            prompt_parts.extend([
                "",
                "=== FULL LOG (for additional context) ===",
                log_content[:2000],  # Limit log content to avoid token limits
            ])
        
        prompt_parts.extend([
            "",
            "=== INSTRUCTIONS ===",
            "1. Analyze the error and identify the root cause",
            "2. Suggest a specific fix with code changes if applicable",
            "3. Explain why this fix should work",
            "4. Provide any additional recommendations",
            "",
            "Please provide your analysis and fix suggestion:",
        ])
        
        return "\n".join(prompt_parts)
    
    def _call_openai(self, prompt: str) -> Dict[str, Any]:
        """Call OpenAI API"""
        try:
            client = openai.OpenAI(
                api_key=self.provider.api_key,
                base_url=self.provider.base_url if self.provider.base_url != 'https://api.openai.com/v1' else None
            )
            
            response = client.chat.completions.create(
                model=self.model.model_id,
                messages=[
                    {"role": "system", "content": "You are a helpful software engineering assistant."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=2000,
            )
            
            content = response.choices[0].message.content
            
            return {
                'suggestion': content,
                'explanation': "Generated by OpenAI",
                'confidence': 'high',
                'model': self.model.model_id,
                'provider': 'openai',
            }
        except Exception as e:
            logger.error(f"Error calling OpenAI: {e}")
            raise
    
    def _call_anthropic(self, prompt: str) -> Dict[str, Any]:
        """Call Anthropic API"""
        try:
            client = anthropic.Anthropic(
                api_key=self.provider.api_key,
                base_url=self.provider.base_url if self.provider.base_url != 'https://api.anthropic.com' else None
            )
            
            message = client.messages.create(
                model=self.model.model_id,
                max_tokens=2000,
                messages=[
                    {"role": "user", "content": prompt}
                ],
            )
            
            content = message.content[0].text if message.content else ""
            
            return {
                'suggestion': content,
                'explanation': "Generated by Anthropic",
                'confidence': 'high',
                'model': self.model.model_id,
                'provider': 'anthropic',
            }
        except Exception as e:
            logger.error(f"Error calling Anthropic: {e}")
            raise
    
    def _call_ollama(self, prompt: str) -> Dict[str, Any]:
        """Call Ollama API (local)"""
        try:
            base_url = self.provider.base_url.rstrip('/')
            url = f"{base_url}/api/generate"
            
            payload = {
                "model": self.model.model_id,
                "prompt": prompt,
                "stream": False,
            }
            
            response = requests.post(url, json=payload, timeout=120)
            response.raise_for_status()
            
            result = response.json()
            content = result.get('response', '')
            
            return {
                'suggestion': content,
                'explanation': "Generated by Ollama (local)",
                'confidence': 'medium',
                'model': self.model.model_id,
                'provider': 'ollama',
            }
        except Exception as e:
            logger.error(f"Error calling Ollama: {e}")
            raise
    
    @staticmethod
    def get_default_model() -> Optional[AIModel]:
        """Get the default AI model"""
        return AIModel.objects.filter(is_default=True, is_active=True).first()
    
    @staticmethod
    def get_model_by_name(model_name: str) -> Optional[AIModel]:
        """Get AI model by display name or model ID"""
        from django.db.models import Q
        return AIModel.objects.filter(
            (Q(display_name=model_name) | Q(model_id=model_name)),
            is_active=True
        ).first()

