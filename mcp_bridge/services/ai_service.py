"""
Unified AI Service for OpenAI, Anthropic, Ollama, and Google Gemini
"""
import openai
import anthropic
import requests
from typing import Dict, Optional, Any
from ..models import LLMProvider, AIModel
import logging

try:
    from google import genai
    GEMINI_AVAILABLE = True
except ImportError:
    # Fallback to old package name for backward compatibility
    try:
        import google.generativeai as genai
        GEMINI_AVAILABLE = True
    except ImportError:
        GEMINI_AVAILABLE = False

logger = logging.getLogger(__name__)


class AIService:
    """Unified service for interacting with different LLM providers"""
    
    def __init__(self, model: AIModel):
        self.model = model
        self.provider = model.provider
    
    def generate_fix_suggestion(
        self,
        error_context: str,
        code_context: Optional[str] = None,
        log_content: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Generate a fix suggestion based on error and code context
        
        Args:
            error_context: The error message or stack trace
            code_context: Relevant code from GitLab repository
            log_content: Full log content for additional context
        
        Returns:
            Dict with 'suggestion', 'explanation', 'confidence'
        """
        prompt = self._build_prompt(error_context, code_context, log_content)
        
        if self.provider.provider_type == 'openai':
            return self._call_openai(prompt)
        elif self.provider.provider_type == 'anthropic':
            return self._call_anthropic(prompt)
        elif self.provider.provider_type == 'ollama':
            return self._call_ollama(prompt)
        elif self.provider.provider_type == 'gemini':
            return self._call_gemini(prompt)
        elif self.provider.provider_type == 'cursor':
            return self._call_cursor(prompt)
        else:
            raise ValueError(f"Unsupported provider type: {self.provider.provider_type}")
    
    def _build_prompt(
        self,
        error_context: str,
        code_context: Optional[str],
        log_content: Optional[str]
    ) -> str:
        """Build the prompt for the LLM"""
        prompt_parts = [
            "You are an expert software engineer analyzing an error in a codebase.",
            "Your task is to identify the root cause and suggest a fix.",
            "",
            "=== ERROR CONTEXT ===",
            error_context,
        ]
        
        if code_context:
            prompt_parts.extend([
                "",
                "=== RELEVANT CODE ===",
                code_context,
            ])
        
        if log_content:
            prompt_parts.extend([
                "",
                "=== FULL LOG (for additional context) ===",
                log_content[:2000],  # Limit log content to avoid token limits
            ])
        
        prompt_parts.extend([
            "",
            "=== INSTRUCTIONS ===",
            "1. Analyze the error and identify the root cause",
            "2. Suggest a specific fix with code changes if applicable",
            "3. Explain why this fix should work",
            "4. Provide any additional recommendations",
            "",
            "Please provide your analysis and fix suggestion:",
        ])
        
        return "\n".join(prompt_parts)
    
    def _call_openai(self, prompt: str) -> Dict[str, Any]:
        """Call OpenAI API"""
        try:
            client = openai.OpenAI(
                api_key=self.provider.api_key,
                base_url=self.provider.base_url if self.provider.base_url != 'https://api.openai.com/v1' else None
            )
            
            response = client.chat.completions.create(
                model=self.model.model_id,
                messages=[
                    {"role": "system", "content": "You are a helpful software engineering assistant."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=2000,
            )
            
            content = response.choices[0].message.content
            
            return {
                'suggestion': content,
                'explanation': "Generated by OpenAI",
                'confidence': 'high',
                'model': self.model.model_id,
                'provider': 'openai',
            }
        except Exception as e:
            logger.error(f"Error calling OpenAI: {e}")
            raise
    
    def _call_anthropic(self, prompt: str) -> Dict[str, Any]:
        """Call Anthropic API"""
        try:
            client = anthropic.Anthropic(
                api_key=self.provider.api_key,
                base_url=self.provider.base_url if self.provider.base_url != 'https://api.anthropic.com' else None
            )
            
            message = client.messages.create(
                model=self.model.model_id,
                max_tokens=2000,
                messages=[
                    {"role": "user", "content": prompt}
                ],
            )
            
            content = message.content[0].text if message.content else ""
            
            return {
                'suggestion': content,
                'explanation': "Generated by Anthropic",
                'confidence': 'high',
                'model': self.model.model_id,
                'provider': 'anthropic',
            }
        except Exception as e:
            logger.error(f"Error calling Anthropic: {e}")
            raise
    
    def _call_ollama(self, prompt: str) -> Dict[str, Any]:
        """Call Ollama API (local)"""
        try:
            base_url = self.provider.base_url.rstrip('/')
            url = f"{base_url}/api/generate"
            
            payload = {
                "model": self.model.model_id,
                "prompt": prompt,
                "stream": False,
            }
            
            response = requests.post(url, json=payload, timeout=120)
            response.raise_for_status()
            
            result = response.json()
            content = result.get('response', '')
            
            return {
                'suggestion': content,
                'explanation': "Generated by Ollama (local)",
                'confidence': 'medium',
                'model': self.model.model_id,
                'provider': 'ollama',
            }
        except Exception as e:
            logger.error(f"Error calling Ollama: {e}")
            raise
    
    def _call_gemini(self, prompt: str) -> Dict[str, Any]:
        """Call Google Gemini API"""
        if not GEMINI_AVAILABLE:
            raise ImportError(
                "google-genai package is not installed. "
                "Install it with: pip install google-genai"
            )
        
        try:
            # Initialize Gemini client
            # New API (google-genai) uses Client, old API (google-generativeai) uses configure
            if hasattr(genai, 'Client'):
                # New API: google-genai
                client = genai.Client(api_key=self.provider.api_key)
                use_new_api = True
            else:
                # Old API: google-generativeai
                genai.configure(api_key=self.provider.api_key)
                client = None
                use_new_api = False
            
            model_id = self.model.model_id
            
            # Try to list available models first (if API supports it)
            available_models = []
            try:
                if use_new_api and hasattr(client, 'list_models'):
                    models = client.list_models()
                    if models:
                        available_models = [m.name if hasattr(m, 'name') else str(m) for m in models]
                        logger.info(f"Available Gemini models: {available_models}")
                elif hasattr(genai, 'list_models'):
                    models = genai.list_models()
                    if models:
                        available_models = [m.name if hasattr(m, 'name') else str(m) for m in models]
                        logger.info(f"Available Gemini models: {available_models}")
            except Exception as e:
                logger.debug(f"Could not list models: {e}")
            
            if not model_id.startswith('models/'):
                # Add models/ prefix if not present
                model_id = f'models/{model_id}'
            
            # Check if the requested model is available
            # If not, and we have available models, try to use a text generation model
            if available_models and model_id not in available_models:
                # Look for text generation models (exclude embedding, imagen, veo, etc.)
                exclude_keywords = ['embedding', 'imagen', 'veo', 'audio', 'tts', 'image', 'preview']
                text_models = [
                    m for m in available_models 
                    if not any(keyword in m.lower() for keyword in exclude_keywords)
                    and ('gemini' in m.lower() or 'flash' in m.lower() or 'pro' in m.lower())
                ]
                if text_models:
                    # Prefer the requested model name if similar exists
                    preferred = [m for m in text_models if model_id.split('/')[-1] in m.lower()]
                    if preferred:
                        model_id = preferred[0]
                        logger.info(f"Using available model: {model_id}")
                    else:
                        logger.warning(f"Requested model {model_id} not available. Using {text_models[0]} instead.")
                        model_id = text_models[0]
                else:
                    # No suitable text model available
                    raise ValueError(
                        f"Gemini model '{model_id}' not found. Available models: {available_models[:5]}... "
                        f"None of them support text generation. Please use Ollama, OpenAI, or Anthropic instead."
                    )
            
            logger.info(f"Attempting to use Gemini model: {model_id}")
            
            # Try new API first (google-genai Client), then old API (GenerativeModel/generate_text)
            content = None
            if use_new_api:
                # New API: google-genai uses Client
                # Try different methods based on what's available
                try:
                    # Method 1: client.models.generate_content
                    if hasattr(client, 'models') and hasattr(client.models, 'generate_content'):
                        response = client.models.generate_content(
                            model=model_id,
                            contents=prompt,
                            config={
                                'temperature': 0.7,
                                'max_output_tokens': 2000,
                            }
                        )
                    # Method 2: client.generate_content directly
                    elif hasattr(client, 'generate_content'):
                        response = client.generate_content(
                            model=model_id,
                            contents=prompt,
                            config={
                                'temperature': 0.7,
                                'max_output_tokens': 2000,
                            }
                        )
                    else:
                        # Fallback: try to get model and generate
                        model = client.get_model(model_id)
                        response = model.generate_content(
                            prompt,
                            generation_config={
                                'temperature': 0.7,
                                'max_output_tokens': 2000,
                            }
                        )
                    
                    # Extract text from response
                    if hasattr(response, 'text'):
                        content = response.text
                    elif hasattr(response, 'candidates') and response.candidates:
                        if hasattr(response.candidates[0], 'content'):
                            if hasattr(response.candidates[0].content, 'parts'):
                                content = response.candidates[0].content.parts[0].text
                            elif hasattr(response.candidates[0].content, 'text'):
                                content = response.candidates[0].content.text
                        else:
                            content = str(response.candidates[0])
                    else:
                        content = str(response)
                except Exception as e:
                    logger.error(f"Error with new API method: {e}")
                    raise
            elif hasattr(genai, 'GenerativeModel'):
                # Old API (google-generativeai >= 0.3.0)
                model = genai.GenerativeModel(model_id)
                response = model.generate_content(
                    prompt,
                    generation_config={
                        'temperature': 0.7,
                        'max_output_tokens': 2000,
                    }
                )
                # Extract text from response
                if hasattr(response, 'text'):
                    content = response.text
                elif hasattr(response, 'candidates') and response.candidates:
                    content = response.candidates[0].content.parts[0].text
                else:
                    content = str(response)
            elif hasattr(genai, 'generate_text'):
                # Old API (google-generativeai 0.1.0rc1)
                response = genai.generate_text(
                    model=model_id,
                    prompt=prompt,
                    temperature=0.7,
                    max_output_tokens=2000,
                )
                # Extract content from old API response
                if hasattr(response, 'result') and response.result:
                    content = response.result
                elif hasattr(response, 'candidates') and response.candidates:
                    candidate = response.candidates[0]
                    if isinstance(candidate, dict) and 'output' in candidate:
                        content = candidate['output']
                    elif hasattr(candidate, 'output'):
                        content = candidate.output
                    else:
                        content = str(candidate)
                else:
                    content = str(response)
            else:
                raise AttributeError(
                    "google-genai package doesn't have GenerativeModel or generate_text. "
                    "Please upgrade: pip install --upgrade google-genai"
                )
            
            # Content is already extracted above based on API version
            # Just verify we have content
            if not content:
                logger.warning(f"Could not extract text from Gemini response: {response}")
                raise ValueError("Gemini API returned empty response")
            
            return {
                'suggestion': content,
                'explanation': "Generated by Google Gemini",
                'confidence': 'high',
                'model': self.model.model_id,
                'provider': 'gemini',
            }
        except Exception as e:
            error_msg = str(e)
            logger.error(f"Error calling Gemini: {e}")
            
            # Provide helpful error message for 404 (model not found)
            if "404" in error_msg or "not found" in error_msg.lower():
                raise ValueError(
                    f"Gemini model '{model_id}' not found. "
                    f"The installed google-generativeai version (0.1.0rc1) is very old and has limited model support. "
                    f"Please either:\n"
                    f"1. Use Ollama (local, free): Configure an Ollama model in Django Admin\n"
                    f"2. Use OpenAI or Anthropic: Configure in Django Admin\n"
                    f"3. Upgrade Python to 3.10+ and install a newer google-generativeai package (when available)\n"
                    f"Current model ID: {self.model.model_id}"
                )
            raise
    
    @staticmethod
    def get_default_model() -> Optional[AIModel]:
        """Get the default AI model"""
        return AIModel.objects.filter(is_default=True, is_active=True).first()
    
    @staticmethod
    def get_model_by_name(model_name: str) -> Optional[AIModel]:
        """Get AI model by display name or model ID"""
        from django.db.models import Q
        return AIModel.objects.filter(
            (Q(display_name=model_name) | Q(model_id=model_name)),
            is_active=True
        ).first()

